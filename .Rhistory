int_kpq <- t(apply(apply(kpq_norm[res_p:1,], 2, cumsum), 1, cumsum))[res_p:1, ]
int_kpq <- int_kpq/max(int_kpq)
# kgrid estimates P(P<=p, Q<=q)/P(Q<=q|H0)
kgrid <- kpq
kgrid$z <- exp(log(int_kpq)-log(int_kqh0))
# avoid 0-0 errors
kgrid$z[which(kgrid$z==0)] <- min(kgrid$z[which(kgrid$z>0)])
if(plot == TRUE){
hist(q_ind, freq = FALSE, xlab = "q", main = "Histogram of q with\nestimated density in red")
lines(kpq$y, kpq_norm[1,], col =  "red")
plot(kpq)
}
# cgrid is grid of cFDR values
# estimated by p/kgrid
cgrid <- vector(mode = "list", length = 3)
cgrid$x = c(seq(0, 0.05, length.out = 101)[1:100], seq(0.05, lims[2], length.out = res_p))
cgrid$y = seq(lims[3], lims[4], length.out = res_q)
ptest = 2*pnorm(-cgrid$x)
cfdrs <- matrix(rep(0, length(cgrid$y)*length(cgrid$x)), ncol = length(cgrid$y))
for (i in 1:length(cgrid$y)) {
xdenom=interp.surface(kgrid,cbind(cgrid$x, rep(cgrid$y[i], length(cgrid$x))))
cfdrs[,i]=cummin(ptest/xdenom)
}
cgrid$z <- cfdrs
# left-censoring
q_grid <- seq(lims[3], lims[4], length.out = res_q)
groups <- cut(q_ind, breaks = q_grid, include.lowest = TRUE, right = TRUE)
# use left point of region where the number of points used for estimation in that bin is >gridp
q_low <- q_grid[which(table(groups)>gridp)[1]]
q_cens <- q
q_cens[which(q_cens < q_low)] <- q_low
# bivariate binning
binned_res <- hexbin(zp, q_cens, xbins = nxbin, IDs = TRUE)
bins <- cbind(binned_res@xcm, binned_res@ycm)
ccut <- interp.surface(cgrid, bins) # cFDR values for binned data
# L-curves are contours of cFDR curves
cl <- lapply(ccut, function(l) grDevices::contourLines(x=cgrid, levels=l))
cl_basic <- cl
# rearrage L-curves so they are defined anticlockwise
lengths <- lapply(cl, length)
for(i in which(lengths>1)){
first = unlist(lapply(cl[[i]], function(x) x$y[1]))
cl[[i]] = cl[[i]][order(first)]
}
# remove contour coords in problematic low q regions
for(i in which(lengths>1)){
cl[[i]][[1]] <- NULL
}
# define newx and newy as joined segments
for(i in 1:length(cl)){
cl[[i]]$newx <- c()
for(j in 1:length(cl[[i]])){
cl[[i]]$newx <- c(cl[[i]]$newx, cl[[i]][[j]]$x)
cl[[i]]$newy <- c(cl[[i]]$newy, cl[[i]][[j]]$y)
}
}
Lx <- lapply(cl, function(x) 2*pnorm(-abs(x$newx)))
Ly <- lapply(cl, function(x) x$newy)
### estimate P(P=p,Q=q|H0)=P(P=q|H0)*P(Q=q|H0)
fph0 = function(x) dunif(x) # P|H0
fqh0 <- approxfun(kpq$y, Pr.q.h0) # Q|H0
fpqh0 <- function(s){
fph0(s[,1])*fqh0(s[,2])
}
# normalise so integral is 1 over full region
fullint <- polyCub(owin(poly=list(x = c(1, 0, 0, 1), y = c(lims[4], lims[4], lims[3], lims[3])), check = FALSE, fix = FALSE), fpqh0, method = "midpoint")
fpqh0 <- function(s){
fph0(s[,1])*fqh0(s[,2])/fullint
}
# integrate bivariate null over L region to get v-vals
v_tmp <- mapply(function(X, Y){
polyCub(owin(poly=list(x = c(X[length(X)], 0,0, c(X[1],X)), y = c(lims[4], lims[4], lims[3], c(lims[3],Y))), check = FALSE, fix  = FALSE), fpqh0, method = "midpoint")
}, X = Lx, Y = Ly)
# map back binned data to original data points
v <- v_tmp[match(binned_res@cID, binned_res@cell)]
v_b4spline <- v
v[which(v==0)] <- 1e-100 # replace any 0 v-vals
if(splinecorr == TRUE){ # spline correction
spline_fit <- bigspline(x = q, y = log10(v/p), nknots = 5, rparm = NA)
#pred_out <- predict.bigspline(spline_fit, newdata = seq(min(q), max(q), 0.05))
distances <- abs(log10(v/p)-spline_fit$fitted.values)
corrected_ind <- which(abs(log10(v/p)-spline_fit$fitted.values) > dist_thr)
v[corrected_ind] <- pmin(10^spline_fit$fitted[corrected_ind]*p[corrected_ind], 1)
}
v[which(v>1)] <- 1 # fix bug where some v vals = 1 + 2.220446e-16
# print warning if v-values have changed too much as something has likely gone wrong
if( median(v) < 0.8*median(p) | median(v) > 1.2*median(p) ) warning('v-values very different to input p-values - check results')
df <- data.frame(p, q, v)
if(splinecorr == TRUE){
return(list(df, data.frame(q_low = q_low, left_cens = length(which(q < q_low)), splinecorr = length(corrected_ind))))
} else
return(list(df, data.frame(q_low = q_low, left_cens = length(which(q < q_low)), splinecorr = NA)))
}
res <- flexible_cfdr(p, q, indep_index = seq(1, n, 1))
indep_index = seq(1, n, 1)
res_p = 300; res_q = 500;nxbin = 1000;gridp = 50; splinecorr = TRUE; dist_thr = 0.5; locfdr_df = 10; plot = TRUE;maf = NULL; check_indep_cor = TRUE; enforce_p_q_cor = TRUE
# match MAF distribution of independent SNPs to that of whole
if(!is.null(maf)) {
if(length(maf) != length(p)) {
stop("Mismatch in lengths of p and maf vectors")
}
indep_index <- match_ind_maf(maf, indep_index)
}
# Suitable for auxiliary covariates other than p-values
if(check_indep_cor) {
if(sign(cor(p[indep_index], q[indep_index], method="spearman"))!= sign(cor(p, q, method="spearman"))) {
stop('Correlation between p and q in whole dataset has a different sign to that in independent subset of SNPs')
}
}
# ensure low q enriched for low p
if(enforce_p_q_cor) {
if(cor(p[indep_index], q[indep_index], method="spearman") < 0) {
q <- -q
}
}
zp = -qnorm(p/2) # convert p-vals to z-scores
# define support for KDE (range of data +/- 10%)
q_10 <- (max(q) - min(q)) * 0.1
zp_10 <- (max(zp) - min(zp)) * 0.1
lims <- c(0, max(zp) + zp_10, min(q) - q_10, max(q) + q_10) # c(xl, xu, yl, yu)
# folded normal KDE only computed for independent SNPs (so BW computation isnt biased)
p_ind <- p[indep_index]
zp_ind <- zp[indep_index]
q_ind <- q[indep_index]
# bivariate density of zp and q
kpq <- MASS::kde2d(c(zp_ind, -zp_ind), c(q_ind, q_ind), n = c(res_p, res_q), lims = lims)
# find optimal mlests parameter values
N = length(c(zp_ind, -zp_ind)); b = 4.3 * exp(-0.26 * log(N, 10)); med = median(c(zp_ind, -zp_ind))
sc = diff(quantile(c(zp_ind, -zp_ind))[c(2,4)])/(2*qnorm(.75))
mlests = locfdr:::locmle(c(zp_ind, -zp_ind), xlim=c(med, b*sc))
names(mlests) = NULL
# local FDR = P(H0|ZP=zp)
lfdr <- tryCatch(
{
locfdr(c(zp_ind, -zp_ind), bre = c(kpq$x[-length(kpq$x)] + diff(kpq$x)/2, kpq$x[length(kpq$x)] + diff(kpq$x)[length(diff(kpq$x))]/2, -c(kpq$x[-length(kpq$x)] + diff(kpq$x)/2, kpq$x[length(kpq$x)] + diff(kpq$x)[length(diff(kpq$x))]/2)), mlests = c(mlests[1], b*mlests[2]), plot = 0, df = locfdr_df)
},
warning=function(cond) {
message("Warning from locfdr:")
message(cond)
message("Try running flexible_cfdr again and adjusting locfdr_df parameter")
}
)
# extract lfdr values for kpq$x values
lfdr_vals <- lfdr$mat[res_p:length(lfdr$mat[,1]),][,2]
lfdrmat <- matrix(lfdr_vals,nrow(kpq$z),ncol(kpq$z))
Pr.pqh0 <- lfdrmat * kpq$z # prob(p,q,H0)
Pr.q.h0 <-  colSums(Pr.pqh0)/sum(Pr.pqh0) # prob(q|H0) = prob(q,H0)/prob(H0)
Pr.cq.h0 <- cumsum(Pr.q.h0) # prob (Q<q | H0)
int_kqh0 <- t(outer(Pr.cq.h0, rep(1, res_p)))
int_kqh0 <- int_kqh0/max(int_kqh0)
cell_size <- (diff(range(lims[1], lims[2])) / res_p) * (diff(range(lims[3], lims[4])) / res_q)
integ <- sum(kpq$z) * cell_size
kpq_norm <- kpq$z/integ
int_kpq <- t(apply(apply(kpq_norm[res_p:1,], 2, cumsum), 1, cumsum))[res_p:1, ]
int_kpq <- int_kpq/max(int_kpq)
# kgrid estimates P(P<=p, Q<=q)/P(Q<=q|H0)
kgrid <- kpq
kgrid$z <- exp(log(int_kpq)-log(int_kqh0))
# avoid 0-0 errors
kgrid$z[which(kgrid$z==0)] <- min(kgrid$z[which(kgrid$z>0)])
if(plot == TRUE){
hist(q_ind, freq = FALSE, xlab = "q", main = "Histogram of q with\nestimated density in red")
lines(kpq$y, kpq_norm[1,], col =  "red")
plot(kpq)
}
# cgrid is grid of cFDR values
# estimated by p/kgrid
cgrid <- vector(mode = "list", length = 3)
cgrid$x = c(seq(0, 0.05, length.out = 101)[1:100], seq(0.05, lims[2], length.out = res_p))
if(plot == TRUE){
hist(q_ind, freq = FALSE, xlab = "q", main = "Histogram of q with\nestimated density in red")
lines(kpq$y, kpq_norm[1,], col =  "red")
#plot(kpq)
}
kpq
image(kpq)
persp(kpq)
if(plot == TRUE){
hist(q_ind, freq = FALSE, xlab = "q", main = "Histogram of q with\nestimated density in red")
lines(kpq$y, kpq_norm[1,], col =  "red")
image(kpq)
}
?image
hist(q)
image(kpq, xlab = "Principal trait Z-scores", ylab = "q")
image(kpq, xlab = "Principal trait Z-scores", ylab = "q", main = "")
image(kpq, xlab = "Principal trait Z-scores", ylab = "q", main = "hi")
if(plot == TRUE){
hist(q_ind, freq = FALSE, xlab = "q", main = "Histogram of q with\nestimated density in red")
lines(kpq$y, kpq_norm[1,], col =  "red")
image(kpq, xlab = "Principal trait Z-scores", ylab = "q", main = "Estimated density from 2D KDE")
}
if(plot == TRUE){
hist(q_ind, freq = FALSE, xlab = "q", main = "Histogram of q with\nestimated density in red")
lines(kpq$y, kpq_norm[1,], col =  "red")
image(kpq, xlab = "Principal trait Z-scores", ylab = "q", main = "Estimated density from 2D KDE")
}
#' @param q continuous auxiliary data values (vector of length n)
#' @param indep_index indices of independent SNPs
#' @param maf minor allele frequencies for SNPs to which \code{p} and \code{q} relate (optional and used to perform MAF matching)
#' @param check_indep_cor check that sign of the correlation between \code{p} and \code{q} is the same in the independent subset as in the whole
#' @param enforce_p_q_cor if \code{p} and \code{q} are negatively correlated, flip the sign on \code{q} values
#'
#' @return
#' @export
#'
#' @examples
zz_in_locfdr <- function(p, q, indep_index, maf = NULL, check_indep_cor = TRUE, enforce_p_q_cor = TRUE){
# match MAF distribution of independent SNPs to that of whole
if(!is.null(maf)) {
if(length(maf) != length(p)) {
stop("Mismatch in lengths of p and maf vectors")
}
indep_index <- match_ind_maf(maf, indep_index)
}
# Suitable for auxiliary covariates other than p-values
if(check_indep_cor) {
if(sign(cor(p[indep_index], q[indep_index], method="spearman"))!= sign(cor(p, q, method="spearman"))) {
stop('Correlation between p and q in whole dataset has a different sign to that in independent subset of SNPs')
}
}
# ensure low q enriched for low p
if(enforce_p_q_cor) {
if(cor(p[indep_index], q[indep_index], method="spearman") < 0) {
q <- -q
}
}
zp = -qnorm(p/2) # convert p-vals to z-scores
# define support for KDE (range of data +/- 10%)
q_10 <- (max(q) - min(q)) * 0.1
zp_10 <- (max(zp) - min(zp)) * 0.1
lims <- c(0, max(zp) + zp_10, min(q) - q_10, max(q) + q_10) # c(xl, xu, yl, yu)
# folded normal KDE only computed for independent SNPs (so BW computation isnt biased)
p_ind <- p[indep_index]
zp_ind <- zp[indep_index]
return(c(zp_ind, -zp_ind))}
zz_in_locfdr(p,q,indep_index = seq(1, n, 1))
message("Warning from locfdr:")
message(cond)
message("Try running flexible_cfdr again and adjusting locfdr_df parameter")
message("...")
message("To determine optimal locfdr_df value see locfdr documentation here: ")
message("https://cran.r-project.org/web/packages/locfdr/vignettes/locfdr-example.pdf")
message("The fcfdr::zz_in_locfdr function can be used to output the zz vector")
message("that flexible_cfdr inputs into locfdr")
message("Warning from locfdr:")
message(cond)
message("Try running flexible_cfdr again and adjusting locfdr_df parameter")
message("...")
message("To determine optimal value for locfdr_df parameter (df in locfdr::locfdr) see locfdr documentation here: ")
message("https://cran.r-project.org/web/packages/locfdr/vignettes/locfdr-example.pdf")
message("The fcfdr::zz_in_locfdr function can be used to output the zz vector")
message("that flexible_cfdr inputs into locfdr internally")
warning('v-values very different to input p-values - check results')
warning('if q has a long tail then try left censoring')
warning('v-values very different to input p-values - check results (if q has a long tail then try left censoring)')
devtools::document()
rm(list = c("flexible_cfdr", "lims", "match_ind_maf", "zz_in_locfdr"))
devtools::document()
devtools::document()
devtools::load_all()
devtools::check()
pkgdown::build_site()
?legend.position
??legend.position
60000/5
#'
#' @param p p-values for principal trait (vector of length n)
#' @param q binary auxiliary data values (vector of length n)
#' @param group group membership of each SNP for leave-one-out procedure (vector of length n) (e.g. chromosome number or LD block)
#'
#' @importFrom Hmisc approxExtrap
#'
#' @return data.frame of p, q and v values
#' @export
#'
binary_cfdr <- function(p, q, group){
unique_group <- unique(group)
# split p and q into groups
p_res <- split(p, f = group)
q_res <- split(q, f = group)
minp=min(p)
maxp=max(p)
# prepare container for v
v_res <- vector(mode = "list", length = length(unique_group))
## prepare x for approxfun
logx=seq(log10(minp),log10(maxp),length.out=1000)
x=c(exp(logx),1)
for(j in 1:length(unique_group)){
this_group <- unique_group[j]
p_loo <- p[-which(group == this_group)] # df leave-one-out
q_loo <- q[-which(group == this_group)] # df leave-one-out
ps <- p_res[[j]]
qs <- q_res[[j]]
q0 <- sum(q_loo == 1 & p_loo > 0.5)/sum(p_loo > 0.5)
mult <- (sum(q_loo == 0 & p_loo > 1/2)/sum(q_loo == 1 & p_loo > 1/2))
q0_sol=sapply(ps, function(p) max(sum(p_loo <= p & q_loo==0),1))
q1_sol=sapply(ps, function(p) max(sum(p_loo <= p & q_loo==1),1))
sol <- ifelse(qs==0,
mult*ps / q0_sol,
(1/mult)*ps / q1_sol)
## approx g0
y=x/sapply(x, function(p) max(sum(p_loo <= p & q_loo==0),1))
extr=Hmisc::approxExtrap(y,x,xout=unique(sol))
invg0=approxfun(x=extr$x,y=pmax(pmin(extr$y,1),0),rule=2)
## invg0_spline=splinefun(x=y,y=x,method="hyman")
## approx g1
y1=x/sapply(x, function(p) max(sum(p_loo <= p & q_loo==1),1))
extr1=Hmisc::approxExtrap(y1,x,xout=unique(sol))
invg1=approxfun(x=extr1$x,y=pmax(pmin(extr1$y,1),0),rule=2)
p1=ifelse(qs==0,invg1(sol),ps)
p0=ifelse(qs==1,invg0(sol),ps)
v_res[[j]] <- p0*(1-q0) + p1*q0
}
data.frame(p = unsplit(p_res, f = group), q = unsplit(q_res, f = group), v = unsplit(v_res, f = group))
}
library(fcfdr)
library(ggplot2)
N <- 1000
p <- runif(N)
q <- rnorm(N)
chr <- rep(c("a", "b"), N/2)
out <- binary_cfdr(p, q>0, chr)
oout
oout
res <- out
ggplot(res, aes(x=p, y=v, color=q)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline()
#'
#' @param p p-values for principal trait (vector of length n)
#' @param q binary auxiliary data values (vector of length n)
#' @param group group membership of each SNP for leave-one-out procedure (vector of length n) (e.g. chromosome number or LD block)
#'
#' @importFrom Hmisc approxExtrap
#'
#' @return data.frame of p, q and v values
#' @export
#'
binary_cfdr <- function(p, q, group){
unique_group <- unique(group)
# split p and q into groups
p_res <- split(p, f = group)
q_res <- split(q, f = group)
minp=min(p)
maxp=max(p)
# prepare container for v
v_res <- vector(mode = "list", length = length(unique_group))
## prepare x for approxfun
logx=seq(log10(minp),log10(maxp),length.out=1000)
x=c(exp(logx),1)
for(j in 1:length(unique_group)){
this_group <- unique_group[j]
p_loo <- p[-which(group == this_group)] # df leave-one-out
q_loo <- q[-which(group == this_group)] # df leave-one-out
ps <- p_res[[j]]
qs <- q_res[[j]]
q0 <- sum(q_loo == 1 & p_loo > 0.5)/sum(p_loo > 0.5)
mult <- (sum(q_loo == 0 & p_loo > 1/2)/sum(q_loo == 1 & p_loo > 1/2))
q0_sol=sapply(ps, function(p) max(sum(p_loo <= p & q_loo==0),1))
q1_sol=sapply(ps, function(p) max(sum(p_loo <= p & q_loo==1),1))
sol <- ifelse(qs==0,
mult*ps / q0_sol,
(1/mult)*ps / q1_sol)
## approx g0
y=x/sapply(x, function(p) max(sum(p_loo <= p & q_loo==0),1))
extr=Hmisc::approxExtrap(y,x,xout=unique(sol))
invg0=approxfun(x=extr$x,y=pmax(pmin(extr$y,1),0),rule=2)
## invg0_spline=splinefun(x=y,y=x,method="hyman")
## approx g1
y1=x/sapply(x, function(p) max(sum(p_loo <= p & q_loo==1),1))
extr1=Hmisc::approxExtrap(y1,x,xout=unique(sol))
invg1=approxfun(x=extr1$x,y=pmax(pmin(extr1$y,1),0),rule=2)
p1=ifelse(qs==0,invg1(sol),ps)
p0=ifelse(qs==1,invg0(sol),ps)
v_res[[j]] <- p0*(1-q0) + p1*q0
}
p = unsplit(p_res, f = group)
q = unsplit(q_res, f = group)
v = unsplit(v_res, f = group)
# correct plateau for low p in applications where p,q are very weakly correlated
if(abs(cor(p,q))<0.01){
# identify problematic points
ind <- which( p < 0.01 & (v < 0.8*p | v > 1.2*p ))
data_bad <- data.frame(p = p[ind], v = v[ind], q = q[ind])
data_good <- data.frame(p = p[-ind], v = v[-ind], q = q[-ind])
# find straight lines from data
lmout_q_0 <- lm(v~p-1, data = data_good[which(data_good$q==0),])
lmout_q_1 <- lm(v~p-1, data = data_good[which(data_good$q==1),])
# replace problematic points
v[ind] <- ifelse(data_bad$q==0, predict(lmout_q_0, data.frame(p = data_bad$p)), predict(lmout_q_1, data.frame(p = data_bad$p)))
}
data.frame(p, q, v)
}
N <- 1000
p <- runif(N)
q <- rnorm(N)
chr <- rep(c("a", "b"), N/2)
res <- binary_cfdr(p, q>0, chr)
ggplot(res, aes(x=p, y=v, color=q)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline()
length(p)
length(p)*0.5
ind = 600
if(ind > length(p)*0.5) warning("p,q have low correlation and >50% of v-values may be problematic - check results")
#'
#' @param p p-values for principal trait (vector of length n)
#' @param q binary auxiliary data values (vector of length n)
#' @param group group membership of each SNP for leave-one-out procedure (vector of length n) (e.g. chromosome number or LD block)
#'
#' @importFrom Hmisc approxExtrap
#'
#' @return data.frame of p, q and v values
#' @export
#'
binary_cfdr <- function(p, q, group){
unique_group <- unique(group)
# split p and q into groups
p_res <- split(p, f = group)
q_res <- split(q, f = group)
minp=min(p)
maxp=max(p)
# prepare container for v
v_res <- vector(mode = "list", length = length(unique_group))
## prepare x for approxfun
logx=seq(log10(minp),log10(maxp),length.out=1000)
x=c(exp(logx),1)
for(j in 1:length(unique_group)){
this_group <- unique_group[j]
p_loo <- p[-which(group == this_group)] # df leave-one-out
q_loo <- q[-which(group == this_group)] # df leave-one-out
ps <- p_res[[j]]
qs <- q_res[[j]]
q0 <- sum(q_loo == 1 & p_loo > 0.5)/sum(p_loo > 0.5)
mult <- (sum(q_loo == 0 & p_loo > 1/2)/sum(q_loo == 1 & p_loo > 1/2))
q0_sol=sapply(ps, function(p) max(sum(p_loo <= p & q_loo==0),1))
q1_sol=sapply(ps, function(p) max(sum(p_loo <= p & q_loo==1),1))
sol <- ifelse(qs==0,
mult*ps / q0_sol,
(1/mult)*ps / q1_sol)
## approx g0
y=x/sapply(x, function(p) max(sum(p_loo <= p & q_loo==0),1))
extr=Hmisc::approxExtrap(y,x,xout=unique(sol))
invg0=approxfun(x=extr$x,y=pmax(pmin(extr$y,1),0),rule=2)
## invg0_spline=splinefun(x=y,y=x,method="hyman")
## approx g1
y1=x/sapply(x, function(p) max(sum(p_loo <= p & q_loo==1),1))
extr1=Hmisc::approxExtrap(y1,x,xout=unique(sol))
invg1=approxfun(x=extr1$x,y=pmax(pmin(extr1$y,1),0),rule=2)
p1=ifelse(qs==0,invg1(sol),ps)
p0=ifelse(qs==1,invg0(sol),ps)
v_res[[j]] <- p0*(1-q0) + p1*q0
}
p = unsplit(p_res, f = group)
q = unsplit(q_res, f = group)
v = unsplit(v_res, f = group)
# correct plateau for low p in applications where p,q are very weakly correlated
if(abs(cor(p,q))<0.01){
# identify problematic points
ind <- which( p < 0.01 & (v < 0.8*p | v > 1.2*p ))
data_bad <- data.frame(p = p[ind], v = v[ind], q = q[ind])
data_good <- data.frame(p = p[-ind], v = v[-ind], q = q[-ind])
# find straight lines from data
lmout_q_0 <- lm(v~p-1, data = data_good[which(data_good$q==0),])
lmout_q_1 <- lm(v~p-1, data = data_good[which(data_good$q==1),])
# replace problematic points
v[ind] <- ifelse(data_bad$q==0, predict(lmout_q_0, data.frame(p = data_bad$p)), predict(lmout_q_1, data.frame(p = data_bad$p)))
if(ind > length(p)*0.5) warning("p,q have low correlation and >50% of v-values may be problematic - check results")
}
data.frame(p, q, v)
}
res <- binary_cfdr(p, q>0, chr)
ggplot(res, aes(x=p, y=v, color=q)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline()
pkgdown::build_site()
pkgdown::build_site()
set.seed(1)
n = 50000
n1p = 500 # associated variants
zp = c(rnorm(n1p, sd=5), rnorm(n-n1p, sd=1)) # z-scores
p = 2*pnorm(-abs(zp)) # convert to p-values
hist(p)
mixture_comp1 <- function(x) rnorm(x, mean = -0.5, sd = 0.5)
mixture_comp2 <- function(x) rnorm(x, mean = 2, sd = 1)
n = length(p)
z = runif(n)
q <- c(mixture_comp1(n1p), mixture_comp2(n-n1p))
hist(q)
corr_plot(p, q)
stratified_qqplot(data_frame = data.frame(p, q), p, q)
stratified_qqplot(data_frame = data.frame(p, q), prin_value_label = p, cond_value_label = q, thresholds = quantiles(q))
data.frame(p, q)
head(data.frame(p, q))
stratified_qqplot(data_frame = data.frame(p, q), prin_value_label = p, cond_value_label = q)
data_frame$negLogP <- -log10(data_frame[, prin_value_label])
quantiles(q)
quantile(q)
stratified_qqplot(data_frame = data.frame(p, q), prin_value_label = p, cond_value_label = q, thresholds = quantile(q))
data_frame = data.frame(p, q)
prin_value_label = p; cond_value_label = q; thresholds = quantile(q)
data_frame$negLogP <- -log10(data_frame[, prin_value_label])
stratified_qqplot(data_frame = data.frame(p, q), prin_value_label = "p", cond_value_label = "q", thresholds = quantile(q))
prin_value_label = "p"; cond_value_label = "q"; thresholds = quantile(q)
data_frame$negLogP <- -log10(data_frame[, prin_value_label])
data_frame
stratified_qqplot(data_frame = data.frame(p, q), prin_value_label = "p", cond_value_label = "q", thresholds = quantile(q)[-1])
length(p)
1108/50000 * 100
109/50000 *100
pkgdown::build_site()
60/3
devtools::document()
rm(list = c("binary_cfdr"))
devtools::document()
pkgdown::build_site
pkgdown::build_site()
